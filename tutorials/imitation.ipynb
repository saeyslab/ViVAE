{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding approximation and PCA initialisation using **imitation**\n",
    "\n",
    "This is a demonstration of **imitation loss**.\n",
    "This approach initialises the ViVAE model parameters according to an existing, trained DR model, effectively biasing the embedding process.\n",
    "Through computing L2 distances between points embedded by the ViVAE model's encoder and their corresponding embedding by the reference model, we quickly optimise the embedding to match the desired starting point before proceeding to train ViVAE fully.\n",
    "We then continue with the rest of the training procedure.\n",
    "\n",
    "If we are training our VAE on PCA-reduced data, we can apply a trivial pre-training using the embedding obtained from taking the top 2 principal components, which we obtain by taking the first two dimensions of the input.\n",
    "This makes the process very fast.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this case study, we PCA-initialise and train a decoder-less ViVAE model to create a 2-dimensional embedding of the [*Reed*](https://cellxgene.cziscience.com/collections/48259aa8-f168-4bf5-b797-af8e88da6637) Human Breast Atlas immune cells dataset.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **0.** Set-up\n",
    "\n",
    "First, let's import required libraries and load in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ViVAE\n",
    "import ViScore\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data below is downloaded from CELLxGENE and pre-processed, as described [here](https://github.com/saeyslab/ViScore/tree/main/benchmarking) and in our manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input          = np.load('../../ViScore/benchmarking/data/Reed_input.npy', allow_pickle=True)\n",
    "input_denoised = np.load('../../ViScore/benchmarking/data/Reed_input_denoised.npy', allow_pickle=True)\n",
    "labs           = np.load('../../ViScore/benchmarking/data/Reed_labels.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction of top 2 principal components from input data is implemented in the `FirstNDimsExtractor` class in ViVAE.\n",
    "**This assumes that the input consists of PCA-reduced data, with columns ordered by decreasing PC eigenvalue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ViVAE.losses import FirstNDimsExtractor\n",
    "ref_model = FirstNDimsExtractor(latent_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.** Pre-training\n",
    "\n",
    "We will now create a ViVAE model and pre-train it to imitate the projection of data obtained from taking the first 2 principal components.\n",
    "To keep things fast, we do not train a decoder (`lam_recon=0.`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "model = ViVAE.ViVAE(input_dim=input.shape[1], random_state=42)\n",
    "model.fit(X=input_denoised, n_epochs=10, batch_size=2048, lam_recon=0., lam_kldiv=1., lam_mds=0., lam_imit=1., ref_model=ref_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the embedding obtained by the pre-trained ViVAE model and low-dimensional PCA are indeed similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pc, ax_pc = ViVAE.plot_embedding(input_denoised[:,range(2)], labs, show=False, figsize=(4,4))\n",
    "ax_pc.get_legend().remove()\n",
    "ax_pc.set_title('Top 2 PCs')\n",
    "fig_pc.savefig('imitation_pc.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb0 = model.transform(input_denoised)\n",
    "fig0, ax0 = ViVAE.plot_embedding(emb0, labs, show=False, figsize=(4,4))\n",
    "ax0.get_legend().remove()\n",
    "ax0.set_title('ViVAE imitating top 2 PCs')\n",
    "fig0.savefig('imitation_vivae0.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.** Continued training\n",
    "\n",
    "We can now continue the training process by optimising stochastic-MDS along with KL divergence from latent prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X=input_denoised, n_epochs=20, batch_size=2048, lam_recon=0., lam_kldiv=1., lam_mds=100., lam_imit=0., ref_model=ref_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = model.transform(input_denoised)\n",
    "fig1, ax1 = ViVAE.plot_embedding(emb1, labs, show=False, figsize=(4,4))\n",
    "ax1.get_legend().remove()\n",
    "ax1.set_title('Fully trained ViVAE')\n",
    "fig1.savefig('imitation_vivae1.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
