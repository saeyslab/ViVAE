{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViVAE workflow for cytometry data\n",
    "\n",
    "This tutorial covers a simple ViVAE analysis pipeline for the Samusik bone marrow CyTOF dataset (DOI: [10.1038/nmeth.3863]((https://pubmed.ncbi.nlm.nih.gov/27183440/))) (from [HDCytoData](https://www.bioconductor.org/packages/release/data/experiment/html/HDCytoData.html)).\n",
    "\n",
    "We demonstrate multiple layers of evaluation and interpretability.\n",
    "\n",
    "In this notebook we\n",
    "\n",
    "- load and preprocess expression data from our dataset of interest\n",
    "\n",
    "- create a 2-dimensional embedding of the data using ViVAE and UMAP\n",
    "\n",
    "- map of manually labelled populations onto embedded points\n",
    "\n",
    "- compute and plot encoder indicatrices to quantify local distortions of the ViVAE latent space\n",
    "\n",
    "- run a FlowSOM (meta)clustering and map the FlowSOM tree onto our ViVAE embedding\n",
    "\n",
    "- use ViScore to asses local and global structure preservation and compare ViVAE to UMAP\n",
    "\n",
    "- save a trained ViVAE model\n",
    "\n",
    "<hr>\n",
    "\n",
    "First let's load all required packages.\n",
    "In addition to ViVAE, we will want the following...\n",
    "\n",
    "FlowSOM for clustering:\n",
    "```\n",
    "pip install git+https://github.com/saeyslab/FlowSOM_Python.git@80529c6b7a1747e8e71042102ac8762c3bfbaa1b\n",
    "```\n",
    "\n",
    "UMAP as an alternative dimension-reduction method:\n",
    "```\n",
    "pip install umap-learn==0.5.5\n",
    "```\n",
    "\n",
    "ViScore for objective structure-preservation scoring:\n",
    "```\n",
    "conda install --channel conda-forge pyemd==1.0.0\n",
    "pip install git+https://github.com/saeyslab/ViScore.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import vivae as vv\n",
    "import umap\n",
    "import flowsom as fs\n",
    "import viscore as vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.** Data import and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import our input FCS file and use an $arcsinh$ transformation to pre-process fluorophore signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff       = fs.io.read_FCS(f'./cytometry_data/Samusik.fcs')\n",
    "col_idcs = np.arange(8, 47) # column indices of markers of interest\n",
    "ff       = ff[:,col_idcs]\n",
    "cofactor = 5.\n",
    "for channel in range(ff.shape[1]):\n",
    "    ff[:,channel].X = np.arcsinh(ff[:,channel].X/cofactor)\n",
    "exprs   = np.asarray(ff.X) # extracted expression data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have manually assigned cell labels available for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels   = np.load(f'./cytometry_data/Samusik_annot.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a *k*-nearest-neighbour graph (*k*-NNG) and use it for data denoising.\n",
    "We do this to boost local structure preservation in ViVAE downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn     = vv.make_knn(x=exprs, fname=f'./cytometry_data/Samusik_knn.npy') # if k-NNG already exists, it is loaded\n",
    "exprs_d = vv.smooth(exprs, knn, k=100, coef=1., n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.** Dimensionality reduction\n",
    "\n",
    "Next we set up a ViVAE model: a parametric dimension-reduction model based on a regularised variational autoencoder (VAE).\n",
    "\n",
    "We train it to learn a non-linear transformation of our high-dimensional (HD) data into a lower-dimensional (LD) smooth latent space, achieving structure-preserving dimensionality reduction.\n",
    "\n",
    "The training is done using a combined loss function consisting of three terms:\n",
    "\n",
    "- **reconstruction error**: measures reconstruction of original data by decoder from LD representation sampled from the latent space\n",
    "\n",
    "- **KL-divergence from latent prior**: divergence from isotropic Gaussian latent prior using reparametrisation to maintain a smooth latent space (read more [here](https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important))\n",
    "\n",
    "- **MDS (quartet) loss**: for joint optimisation of relative intra-quartet distances between repeatedly and randomly sampled points (see [SQuadMDS](https://dial.uclouvain.be/pr/boreal/object/boreal:264665) for original idea behind this); **for cytometry data we typically set the weight of this loss to 10 (vs. the other two errors)**\n",
    "\n",
    "There are many training options.\n",
    "You can read about them in the documentation by running:\n",
    "```\n",
    "help(ViVAE.ViVAE)\n",
    "```\n",
    "\n",
    "Some tips on hyperparameter tuning:\n",
    "\n",
    "- Increase number of epochs (`n_epochs`) for better convergence or decrease it for shorter running times.\n",
    "\n",
    "- Increase batch size (`batch_size`) for faster training, decrease it for less memory usage.\n",
    "\n",
    "- Optimiser parameters (`learning_rate` and `weight_decay`) can be increased to speed up training, at the risk of instability and sub-optimal solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "model = vv.ViVAE(input_dim=exprs.shape[1], latent_dim=2)\n",
    "model.fit(exprs_d, n_epochs=50, lam_mds=10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now easily obtain a 2-dimensional embedding of our dataset and plot it using a colour scheme for points based on the manually assigned cell populations we imported earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.transform(exprs_d)\n",
    "vv.plot_embedding(embedding=emb, labels=labels, unassigned='unassigned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.** Integration with FlowSOM\n",
    "\n",
    "FlowSOM is one of the go-to methods for analyses of cytometry datasets.\n",
    "Thanks to this, many researchers are used to interpreting the outputs of FlowSOM analyses.\n",
    "If you are one of them, you can apply a FlowSOM analysis to the original expression data and see how concordant the ViVAE embedding is with your FlowSOM tree.\n",
    "\n",
    "First, let's train a FlowSOM model using the imported FCS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsom = fs.FlowSOM(ff, n_clusters=40, xdim=10, ydim=10, seed=42) # train FlowSOM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check how the minimum spanning tree (MST) from FlowSOM maps onto our embedding.\n",
    "\n",
    "It is easy to plot the composition of each cluster in terms of manually defined populations, the expression of one or more markers of interest in each cluster or the mapping of each node to its cluster/metacluster.\n",
    "\n",
    "The code below shows compositions of clusters in terms of defined populations.\n",
    "(Set the `fsom_plot_unassigned` argument to include unlabelled cells in the pie charts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.plot_embedding(embedding=emb, labels=labels, unassigned='unassigned', fsom=fsom, dr_model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot relative expression of multiple selected markers of interest, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.plot_embedding(embedding=emb, labels=labels, unassigned='unassigned', fsom=fsom, dr_model=model, fsom_view='markers', fsom_markers=['CD4', 'CD8'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in only a single marker, the plot becomes simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.plot_embedding(embedding=emb, labels=labels, unassigned='unassigned', fsom=fsom, dr_model=model, fsom_view='markers', fsom_markers=['CD34'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot metacluster numbers, which can be practical for downstream analyses on the metacluster level (eg. differential expression analyses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.plot_embedding(embedding=emb, labels=labels, unassigned='unassigned', fsom=fsom, dr_model=model, fsom_view='metaclusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read about all the FlowSOM plotting options in the documentation:\n",
    "```\n",
    "help(ViVAE.plot_embedding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.** Encoder indicatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we compute and plot the encoder indicatrices, *ie.* objective indicators of local directed stretching of the latent space.\n",
    "These are based on, but separate and different from, decoder indicatrices as implemented in *Geometric Autoencoders* (DOI: [arXiv:2306.17638](https://arxiv.org/abs/2306.17638)).\n",
    "\n",
    "Indicatrices are small, equally large hyperspheres in the original, high-dimensional (HD) space (they lie in the [horizontal tangent spaces](https://gemini.google.com/share/9d567cee0c08) of existing data points).\n",
    "\n",
    "As they are passed through the encoder, they can get distorted in two different ways.\n",
    "Firstly, their size can vary across the embedding, including relative stretching or contraction of some parts of the points cloud.\n",
    "Secondly, their shape can change (from sphere to ellipsis), which indicates the specific direction along which the distortion occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_nice = model.encoder_indicatrices(X=exprs_d, radius=1e-3, n_steps=50, n_polygon=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.plot_indicatrices(indicatrices=ei_nice, scale_factor=8e1, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperspheres should have a small radius (`radius` in `model.encoder_indicatrices`), but not so small that the resulting shapes in the plot have sharp, as opposed to smooth, edges.\n",
    "\n",
    "If the radius is set too large, the resulting plot may show a deformed elliptical, C-like or S-like shape in place of an ellipsis.\n",
    "\n",
    "This is because the hyperspheres reach too far away from the data points and capture deformation outside the horizontal tangent space.\n",
    "Such a visualisation is typically less informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_large = model.encoder_indicatrices(X=exprs_d, radius=1e2, n_steps=50, n_polygon=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.plot_indicatrices(indicatrices=ei_large, scale_factor=3e-3, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the radius is set too small, the edges of the hyperspheres will appear sharp, as opposed to smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_small = model.encoder_indicatrices(X=exprs_d, radius=1e-6, n_steps=50, n_polygon=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.plot_indicatrices(indicatrices=ei_small, scale_factor=1e5, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `scale_factor` in `plot_indicatrices` needs to be adapted for a good plot (*ie.* indicatrices should be plotted large enough to be informative, but small enough to not have too much overlap).\n",
    "\n",
    "These are some additional arguments you can alter:\n",
    "\n",
    "- `n_steps` in `encoder_indicatrices` determines the density of indicatrices (which are sampled on a grid). Higher values will ensure better coverage of the space, but cause a more cluttered plot.\n",
    "\n",
    "- `n_polygon` in `encoder_indicatrices` determined how many points per indicatrix are sampled. Higher values will ensure higher-quality graphical output, but may increase running time if extremely high.\n",
    "\n",
    "Note that encoder indicatrices can be implemented for any encoder (*ie.* differentiable dimension-reducing models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.** Objective evaluation of structure preservation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ViScore allow us to evaluate Local and Global Structure Preservation (SP).\n",
    "This is achieved through accurate approximation of $R_{NX}$ curves, making it possible to measure multi-scale structure preservation in an objective manner, even for large datasets for which this was previously impossible.\n",
    "\n",
    "Let's create a UMAP embedding of the same dataset and compare the performance of UMAP and of ViVAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_umap = umap.UMAP()\n",
    "model_umap.fit(exprs)\n",
    "emb_umap = model_umap.transform(exprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ViVAE plotting function can be used to display the UMAP embedding also.\n",
    "Since this implementation of UMAP has a `.transform` method (*ie.* new points can be embedded into the latent space after training), we can even map the same FlowSOM tree onto the embedding as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv.plot_embedding(emb_umap, labels=labels, unassigned='unassigned', fsom=fsom, dr_model=model_umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute Local and Global Structure Preservation (SP) for the UMAP and the ViVAE embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare embeddings w.r.t. original input into each method\n",
    "s_vivae = vs.score(hd=exprs, ld=emb)\n",
    "s_umap  = vs.score(hd=exprs, ld=emb_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Report ViVAE and UMAP scores\n",
    "print(f'ViVAE:\\n\\tLocal SP:\\t{s_vivae[\"Sl\"]:.3f}\\n\\tGlobal SP:\\t{s_vivae[\"Sg\"]:.3f}')\n",
    "print(f'UMAP:\\n\\tLocal SP:\\t{s_umap[\"Sl\"]:.3f}\\n\\tGlobal SP:\\t{s_umap[\"Sg\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.** Saving and loading a ViVAE model\n",
    "\n",
    "You might want to save your trained ViVAE model to apply it to new data, or even continue training it, later.\n",
    "The easiest way to do this is to use the standard `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model.pkl', 'wb') as output:\n",
    "  pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load your model back up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as input:\n",
    "    model = pickle.load(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
